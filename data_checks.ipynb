{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdeb6fb2",
   "metadata": {},
   "source": [
    "# Data & Tokenizer Checks for dimabsa\n",
    "\n",
    "This notebook runs a compact set of checks to ensure the dataset and tokenizer are consistent and sane for training. It focuses on:\n",
    "- tokenizer special tokens and segment ids\n",
    "- sample encodings for `text` and `(text, target)` pairs\n",
    "- label shapes and statistics\n",
    "- simple data quality checks (malformed lines, default labels)\n",
    "\n",
    "Run cells sequentially. If you want, I can execute these cells and show the outputs for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bc3127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Imports & helper utilities\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# Helper utils\n",
    "\n",
    "def print_heading(text):\n",
    "    print(\"\\n\" + \"#\"*6 + \" \" + text + \" \" + \"#\"*6 + \"\\n\")\n",
    "\n",
    "\n",
    "def assert_ok(cond, msg):\n",
    "    if not cond:\n",
    "        raise AssertionError(msg)\n",
    "    else:\n",
    "        print(\"OK: \", msg)\n",
    "\n",
    "\n",
    "print(\"Imports done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0571fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Environment & dependency versions\n",
    "\n",
    "print_heading(\"Environment & Versions\")\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Torch:\", torch.__version__)\n",
    "try:\n",
    "    import transformers\n",
    "    print(\"Transformers:\", transformers.__version__)\n",
    "except Exception:\n",
    "    print(\"Transformers not installed or can't be imported\")\n",
    "\n",
    "try:\n",
    "    import sklearn\n",
    "    print(\"sklearn:\", sklearn.__version__)\n",
    "except Exception:\n",
    "    print(\"sklearn not available\")\n",
    "\n",
    "print_heading(\"Working directory and key files\")\n",
    "print(\"CWD:\", os.getcwd())\n",
    "print(\"Data file exists:\", Path('data/eng_laptop_train_alltasks.jsonl').exists())\n",
    "print(\"Dataloader exists:\", Path('dataloader.py').exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea84e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Setup variables and load dataset\n",
    "\n",
    "# Defaults (change if you used a different path/model)\n",
    "DATA_PATH = Path('data/eng_laptop_train_alltasks.jsonl')\n",
    "MODEL_NAME = 'microsoft/deberta-v3-large'\n",
    "\n",
    "print_heading('Paths & Model')\n",
    "print('DATA_PATH:', DATA_PATH)\n",
    "print('MODEL_NAME:', MODEL_NAME)\n",
    "\n",
    "# Import Dataloader from the local file\n",
    "from dataloader import Dataloader\n",
    "\n",
    "# Parse and instantiate\n",
    "print_heading('Parsing data (this uses Dataloader._parse_jsonl)')\n",
    "full_data = Dataloader._parse_jsonl(str(DATA_PATH))\n",
    "print('Total flattened entries:', len(full_data))\n",
    "\n",
    "print('\\nShow 3 sample flattened entries:')\n",
    "for i, e in enumerate(full_data[:3]):\n",
    "    print(f\"--- Sample {i} ---\")\n",
    "    pprint(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df56ff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Tokenizer sanity checks and sample encodings\n",
    "\n",
    "print_heading('Tokenizer & encoding checks')\n",
    "\n",
    "# Instantiate dataset (this will create a tokenizer inside Dataloader)\n",
    "train_ds, val_ds = Dataloader.prepare_splits(str(DATA_PATH), MODEL_NAME)\n",
    "\n",
    "print('Train size, Val size:', len(train_ds), len(val_ds))\n",
    "\n",
    "# Access tokenizer\n",
    "tokenizer = train_ds.tokenizer\n",
    "print('Special tokens:')\n",
    "print('CLS token:', tokenizer.cls_token, 'id:', tokenizer.cls_token_id)\n",
    "print('SEP token:', tokenizer.sep_token, 'id:', tokenizer.sep_token_id)\n",
    "print('Pad token:', tokenizer.pad_token, 'id:', tokenizer.pad_token_id)\n",
    "\n",
    "# Show encoding for a sample\n",
    "sample_idx = 0\n",
    "sample_row = train_ds.data[sample_idx]\n",
    "print('\\nSample raw text and target:')\n",
    "print('Text:', sample_row['Text'])\n",
    "print('Target:', sample_row['Target'])\n",
    "print('Labels (Valence, Arousal):', sample_row['Valence'], sample_row['Arousal'])\n",
    "\n",
    "enc_pair = tokenizer(str(sample_row['Text']), str(sample_row['Target']), max_length=128, padding='max_length', truncation=True, return_tensors=None)\n",
    "enc_text = tokenizer(str(sample_row['Text']), max_length=128, padding='max_length', truncation=True, return_tensors=None)\n",
    "\n",
    "print('\\nEncoding keys:', list(enc_pair.keys()))\n",
    "print('First 50 input_ids (pair):', enc_pair['input_ids'][:50])\n",
    "print('First 50 attention_mask (pair):', enc_pair['attention_mask'][:50])\n",
    "if 'token_type_ids' in enc_pair:\n",
    "    print('First 50 token_type_ids (pair):', enc_pair['token_type_ids'][:50])\n",
    "\n",
    "# Convert ids to tokens for the first 50 tokens\n",
    "tokens_pair = tokenizer.convert_ids_to_tokens(enc_pair['input_ids'][:50])\n",
    "print('\\nTokens (pair) first 50:\\n', tokens_pair)\n",
    "\n",
    "# Compare text-only encoding to pair encoding around the separator\n",
    "ids_text = enc_text['input_ids']\n",
    "ids_pair = enc_pair['input_ids']\n",
    "\n",
    "# Find SEP tokens positions (if available)\n",
    "sep_id = tokenizer.sep_token_id\n",
    "sep_positions = [i for i, idv in enumerate(ids_pair) if idv == sep_id]\n",
    "print('\\nSEP positions in pair encoding:', sep_positions)\n",
    "\n",
    "# Show the sub-token sequences around sep tokens\n",
    "for pos in sep_positions:\n",
    "    start = max(0, pos-10)\n",
    "    end = min(len(ids_pair), pos+10)\n",
    "    print('\\nContext around SEP at pos', pos, ':')\n",
    "    print(tokenizer.convert_ids_to_tokens(ids_pair[start:end]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e26ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Labels & statistics\n",
    "\n",
    "print_heading('Labels statistics')\n",
    "\n",
    "vals = [ (row['Valence'], row['Arousal']) for row in train_ds.data ]\n",
    "val_arr = np.array(vals)\n",
    "\n",
    "print('Valence stats: mean=%.3f std=%.3f min=%.3f max=%.3f' % (val_arr[:,0].mean(), val_arr[:,0].std(), val_arr[:,0].min(), val_arr[:,0].max()))\n",
    "print('Arousal stats: mean=%.3f std=%.3f min=%.3f max=%.3f' % (val_arr[:,1].mean(), val_arr[:,1].std(), val_arr[:,1].min(), val_arr[:,1].max()))\n",
    "\n",
    "# Count default labels (5.0, 5.0) vs parsed\n",
    "default_count = sum(1 for v,a in vals if v == 5.0 and a == 5.0)\n",
    "print('Default labels (5.0,5.0) count:', default_count)\n",
    "\n",
    "# Show distribution of unique targets\n",
    "from collections import Counter\n",
    "targets = [row['Target'] for row in train_ds.data]\n",
    "print('\\nNumber of unique targets:', len(set(targets)))\n",
    "print('Most common targets:')\n",
    "for t,c in Counter(targets).most_common(10):\n",
    "    print(f\"  {t}: {c}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc8fc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Consistency check: tokenizer vs model in train.py\n",
    "\n",
    "print_heading('Consistency check with train.py')\n",
    "\n",
    "# Try to read train.py argument default for model (best-effort)\n",
    "train_model_name = None\n",
    "try:\n",
    "    import ast\n",
    "    with open('train.py', 'r') as f:\n",
    "        tree = ast.parse(f.read())\n",
    "        # crude search for a default model_name arg\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.Call) and getattr(node.func, 'id', '') == 'add_argument':\n",
    "                args = [a.s for a in node.args if isinstance(a, ast.Str)]\n",
    "                if args and '--model_name' in args:\n",
    "                    for kw in node.keywords:\n",
    "                        if kw.arg == 'default':\n",
    "                            train_model_name = kw.value.s\n",
    "\n",
    "except Exception:\n",
    "    train_model_name = None\n",
    "\n",
    "print('Model name in this notebook (MODEL_NAME):', MODEL_NAME)\n",
    "print('Detected default in train.py:', train_model_name)\n",
    "if train_model_name and train_model_name != MODEL_NAME:\n",
    "    print('\\nWARNING: model name used for tokenizer is different from train.py default. Use the same model for tokenizer and model weights to avoid mismatch.')\n",
    "else:\n",
    "    print('\\nModel names appear consistent or could not be determined from train.py')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46815795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Simple data quality tests (assertions)\n",
    "\n",
    "print_heading('Quick assertion tests')\n",
    "\n",
    "# test 1: dataset non-empty\n",
    "assert_ok(len(train_ds) > 0, 'train dataset not empty')\n",
    "\n",
    "# test 2: labels shape and range\n",
    "sample_lab = train_ds[0]['labels']\n",
    "assert_ok(isinstance(sample_lab, torch.Tensor) and sample_lab.shape[0] == 2, 'labels are a torch tensor of length 2')\n",
    "assert_ok((sample_lab >= 0).all().item() and (sample_lab <= 10).all().item(), 'labels in reasonable range (0-10)')\n",
    "\n",
    "# test 3: token_type_ids presence is optional but we print a note\n",
    "if 'token_type_ids' not in train_ds[0]:\n",
    "    print('NOTE: token_type_ids not present in encoding. This is OK for many models (e.g., Deberta). If your model expects token_type_ids, switch to a model that supports them or add them.')\n",
    "else:\n",
    "    print('token_type_ids present: first 20 ->', train_ds[0]['token_type_ids'][:20])\n",
    "\n",
    "print('\\nAll quick assertions passed (if no AssertionError above).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6977214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Save a small sample of encodings to disk and reload to validate save/load\n",
    "\n",
    "print_heading('Save / Load validation')\n",
    "\n",
    "to_save = {\n",
    "    'sample_idx': sample_idx,\n",
    "    'raw': sample_row,\n",
    "    'enc_pair': enc_pair,\n",
    "}\n",
    "\n",
    "out_path = Path('logs') / 'sample_encoding.json'\n",
    "out_path.parent.mkdir(exist_ok=True)\n",
    "with open(out_path, 'w', encoding='utf-8') as fh:\n",
    "    json.dump(to_save, fh, indent=2, ensure_ascii=False)\n",
    "\n",
    "print('Saved sample to', out_path)\n",
    "with open(out_path, 'r', encoding='utf-8') as fh:\n",
    "    loaded = json.load(fh)\n",
    "\n",
    "assert_ok('raw' in loaded and 'enc_pair' in loaded, 'saved JSON contains expected keys')\n",
    "print('Save/load OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a8318d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Reproducibility check: seeding\n",
    "\n",
    "print_heading('Reproducibility checks (seeding)')\n",
    "\n",
    "random.seed(1234)\n",
    "np.random.seed(1234)\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# sample random indices twice and assert same\n",
    "idxs1 = np.random.randint(0, len(train_ds), size=5)\n",
    "np.random.seed(1234)\n",
    "idxs2 = np.random.randint(0, len(train_ds), size=5)\n",
    "assert_ok(np.array_equal(idxs1, idxs2), 'random sampling reproducible with seed')\n",
    "print('Example indices:', idxs1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2847d9",
   "metadata": {},
   "source": [
    "# 10) Final guidance\n",
    "\n",
    "print_heading('Guidance & next steps')\n",
    "print('If you see:')\n",
    "print(\" - token_type_ids missing: this is fine for many DeBERTa-like models; only BERT needs them.\")\n",
    "print(\" - different tokenizer vs model: use the same model name for tokenizer and model weights (pass args.model_name into Dataloader.prepare_splits)\")\n",
    "print(\" - constant labels (e.g., many 5.0): check source data parsing and the 'VA' field parsing logic\")\n",
    "print(\" - labels out of expected range: make sure Valence/Arousal parsing uses correct separators and casts to float\")\n",
    "\n",
    "print('\\nIf you want, I can execute these cells and show the outputs for your project â€” say `run it` and I will run the key cells and paste their outputs here.')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
